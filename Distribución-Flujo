Ventajas de esta distribución:
Load balancing natural:

Server 1 maneja inference pesado
Server 2 maneja datos y embeddings
Ambas GPUs trabajando en paralelo

Redundancia:

Si un server falla, el otro puede tomar carga básica
Datos separados del procesamiento

Flujo optimizado:

Query llega → NestJS (Server 1)
Genera embeddings → Ollama Embeddings (Server 2)
Busca vectores → Qdrant (Server 2)
Consulta BBDD → PostgreSQL (Server 2)
Inference final → Ollama Principal (Server 1)
Respuesta → Usuario

Configuración de red:
Load balancer simple con nginx o HAProxy en uno de los servers para distribuir requests.
Modelos recomendados por GPU:
P2000 (5GB) aguanta cómodamente:

Llama 3.1 7B (4GB VRAM)
Mistral 7B (4GB VRAM)
Phi-3 Medium (4GB VRAM)

Para embeddings:

nomic-embed-text (muy eficiente en GPU)
all-MiniLM en CPU del Server 2


Distribución

Server 1 (i5 + 32GB + P2000) - "Procesamiento"
VM 1: Ollama Principal (GPU dedicada)
  - Modelo principal (Llama 3.1 8B o 7B)
  - 16GB RAM asignada
  - GPU P2000 pasthrough

CT 1: NestJS Backend + Embeddings
  - Sentence Transformers (all-MiniLM)
  - Procesamiento de documentos
  - Orquestación RAG
  - 8GB RAM
Server 2 (i7 6700 + 16GB + P2000) - "Datos"
VM 1: Ollama Secundario (para embeddings locales)
  - Modelo de embeddings (nomic-embed-text)
  - 8GB RAM asignada
  - GPU P2000 para embeddings rápidos

CT 1: Qdrant Vector Database
  - Base de datos vectorial
  - 4GB RAM

CT 2: PostgreSQL 
  - Tu BBDD actual
  - 4GB RAM
