"use strict";
var __decorate = (this && this.__decorate) || function (decorators, target, key, desc) {
    var c = arguments.length, r = c < 3 ? target : desc === null ? desc = Object.getOwnPropertyDescriptor(target, key) : desc, d;
    if (typeof Reflect === "object" && typeof Reflect.decorate === "function") r = Reflect.decorate(decorators, target, key, desc);
    else for (var i = decorators.length - 1; i >= 0; i--) if (d = decorators[i]) r = (c < 3 ? d(r) : c > 3 ? d(target, key, r) : d(target, key)) || r;
    return c > 3 && r && Object.defineProperty(target, key, r), r;
};
var __metadata = (this && this.__metadata) || function (k, v) {
    if (typeof Reflect === "object" && typeof Reflect.metadata === "function") return Reflect.metadata(k, v);
};
var __param = (this && this.__param) || function (paramIndex, decorator) {
    return function (target, key) { decorator(target, key, paramIndex); }
};
var OllamaProxyController_1;
Object.defineProperty(exports, "__esModule", { value: true });
exports.OllamaProxyController = void 0;
const common_1 = require("@nestjs/common");
const rag_service_1 = require("../../services/rag/rag.service");
const ollama_util_1 = require("../../utils/ollama.util");
let OllamaProxyController = OllamaProxyController_1 = class OllamaProxyController {
    ragService;
    logger = new common_1.Logger(OllamaProxyController_1.name);
    constructor(ragService) {
        this.ragService = ragService;
    }
    async generate(body) {
        try {
            this.logger.log(`=== OLLAMA GENERATE REQUEST ===`);
            this.logger.log(`Request body: ${JSON.stringify(body, null, 2)}`);
            const prompt = body.prompt || '';
            const model = body.model || 'llama3.1:8b';
            if (!prompt) {
                throw new Error('No prompt found');
            }
            const result = await this.ragService.processQuery(prompt, 5, 0.3, model);
            const response = {
                model: model,
                created_at: new Date().toISOString(),
                response: result.response,
                done: true,
                context: result.sources.map(s => s.payload.fileName),
                total_duration: result.processingTime * 1000000,
                load_duration: 0,
                prompt_eval_count: prompt.length,
                prompt_eval_duration: 1000000,
                eval_count: result.response.length,
                eval_duration: result.processingTime * 1000000,
            };
            this.logger.log(`=== SUCCESS ===`);
            this.logger.log(`Sources found: ${result.sources.length}`);
            return response;
        }
        catch (error) {
            this.logger.error('=== ERROR ===', error.message);
            return {
                model: body.model || 'llama3.1:8b',
                created_at: new Date().toISOString(),
                response: `Error: ${error.message}`,
                done: true,
                error: error.message,
            };
        }
    }
    async chat(body, req) {
        try {
            const messages = body.messages || [];
            const lastMessage = messages[messages.length - 1];
            const prompt = lastMessage?.content || '';
            const model = body.model || 'llama3.1:8b';
            const isAutoGenerated = prompt.includes('### Task:') ||
                prompt.includes('follow-up questions') ||
                prompt.includes('Generate a concise') ||
                prompt.includes('Generate 1-3 broad tags');
            if (isAutoGenerated) {
                this.logger.log(`=== SKIPPING AUTO-GENERATED REQUEST ===`);
                this.logger.log(`Type: ${prompt.substring(0, 50)}...`);
                return {
                    model: model,
                    created_at: new Date().toISOString(),
                    response: '{}',
                    done: true,
                    total_duration: 100000000,
                };
            }
            this.logger.log(`=== API CHAT REQUEST (USER) ===`);
            this.logger.log(`Prompt: "${prompt}"`);
            this.logger.log(`Stream: ${body.stream}`);
            if (!prompt) {
                throw new Error('No prompt found in messages');
            }
            const result = await this.ragService.processQuery(prompt, 5, 0.3, model);
            if (body.stream) {
                const streamResponse = {
                    model: model,
                    created_at: new Date().toISOString(),
                    message: {
                        role: 'assistant',
                        content: result.response,
                    },
                    done: true,
                    total_duration: result.processingTime * 1000000,
                    load_duration: 0,
                    prompt_eval_count: prompt.length,
                    prompt_eval_duration: 1000000,
                    eval_count: result.response.length,
                    eval_duration: result.processingTime * 1000000,
                };
                this.logger.log(`=== STREAMING SUCCESS ===`);
                this.logger.log(`Response: ${result.response.substring(0, 100)}...`);
                this.logger.log(`Sources: ${result.sources.length}`);
                return streamResponse;
            }
            const ollamaResponse = {
                model: model,
                created_at: new Date().toISOString(),
                response: result.response,
                done: true,
                context: result.sources.map(s => s.payload.fileName),
                total_duration: result.processingTime * 1000000,
                load_duration: 0,
                prompt_eval_count: prompt.length,
                prompt_eval_duration: 1000000,
                eval_count: result.response.length,
                eval_duration: result.processingTime * 1000000,
            };
            this.logger.log(`=== NON-STREAMING SUCCESS ===`);
            this.logger.log(`Response: ${result.response.substring(0, 100)}...`);
            this.logger.log(`Sources: ${result.sources.length}`);
            return ollamaResponse;
        }
        catch (error) {
            this.logger.error('=== ERROR IN API CHAT ===', error.message);
            return {
                model: body.model || 'llama3.1:8b',
                created_at: new Date().toISOString(),
                response: `Error: ${error.message}`,
                done: true,
                error: error.message,
            };
        }
    }
    async chatCompleted(body) {
        this.logger.log(`=== CHAT COMPLETED ===`);
        return { status: 'completed' };
    }
    async chatCompletions(body, req) {
        try {
            this.logger.log(`=== OPENAI CHAT COMPLETIONS ===`);
            this.logger.log(`Request body: ${JSON.stringify(body, null, 2)}`);
            this.logger.log(`Stream: ${body.stream}`);
            const messages = body.messages || [];
            const lastMessage = messages[messages.length - 1];
            const prompt = lastMessage?.content || '';
            const model = body.model || 'llama3.1:8b';
            if (!prompt) {
                throw new Error('No prompt found in messages');
            }
            const result = await this.ragService.processQuery(prompt, 5, 0.3, model);
            const ollamaResponse = {
                model: model,
                created_at: new Date().toISOString(),
                response: result.response,
                done: true,
                context: result.sources.map(s => s.payload.fileName),
                total_duration: result.processingTime * 1000000,
                load_duration: 0,
                prompt_eval_count: prompt.length,
                prompt_eval_duration: 1000000,
                eval_count: result.response.length,
                eval_duration: result.processingTime * 1000000,
            };
            this.logger.log(`=== CHAT COMPLETIONS SUCCESS ===`);
            this.logger.log(`Sources: ${result.sources.length}`);
            return ollamaResponse;
        }
        catch (error) {
            this.logger.error('=== ERROR IN CHAT COMPLETIONS ===', error.message);
            return {
                model: body.model || 'llama3.1:8b',
                created_at: new Date().toISOString(),
                response: `Error: ${error.message}`,
                done: true,
                error: error.message,
            };
        }
    }
    async listModels() {
        try {
            this.logger.log(`=== GETTING MODELS ===`);
            const models = await ollama_util_1.OllamaUtil.listModels();
            const response = {
                models: models.map(name => ({
                    name,
                    model: name,
                    size: 4000000000,
                    digest: 'sha256:fake',
                    details: { format: 'gguf', family: 'llama' },
                    modified_at: new Date().toISOString(),
                })),
            };
            this.logger.log(`=== MODELS RESPONSE ===`);
            this.logger.log(`Found ${models.length} models`);
            return response;
        }
        catch (error) {
            this.logger.error('=== ERROR GETTING MODELS ===', error);
            return { models: [] };
        }
    }
    async getVersion() {
        this.logger.log(`=== VERSION REQUEST ===`);
        return { version: '0.1.0-nestjs-rag' };
    }
    async debugCatchAll(body, req) {
        this.logger.log(`=== CATCH-ALL API REQUEST ===`);
        this.logger.log(`Method: ${req.method}`);
        this.logger.log(`URL: ${req.url}`);
        this.logger.log(`Path: ${req.path}`);
        this.logger.log(`Body: ${JSON.stringify(body, null, 2)}`);
        return {
            message: 'Caught by NestJS catch-all - endpoint not found',
            method: req.method,
            url: req.url,
            path: req.path,
            body: body,
        };
    }
};
exports.OllamaProxyController = OllamaProxyController;
__decorate([
    (0, common_1.Post)('api/generate'),
    __param(0, (0, common_1.Body)()),
    __metadata("design:type", Function),
    __metadata("design:paramtypes", [Object]),
    __metadata("design:returntype", Promise)
], OllamaProxyController.prototype, "generate", null);
__decorate([
    (0, common_1.Post)('api/chat'),
    __param(0, (0, common_1.Body)()),
    __param(1, (0, common_1.Req)()),
    __metadata("design:type", Function),
    __metadata("design:paramtypes", [Object, Object]),
    __metadata("design:returntype", Promise)
], OllamaProxyController.prototype, "chat", null);
__decorate([
    (0, common_1.Post)('api/chat/completed'),
    __param(0, (0, common_1.Body)()),
    __metadata("design:type", Function),
    __metadata("design:paramtypes", [Object]),
    __metadata("design:returntype", Promise)
], OllamaProxyController.prototype, "chatCompleted", null);
__decorate([
    (0, common_1.Post)('api/chat/completions'),
    __param(0, (0, common_1.Body)()),
    __param(1, (0, common_1.Req)()),
    __metadata("design:type", Function),
    __metadata("design:paramtypes", [Object, Object]),
    __metadata("design:returntype", Promise)
], OllamaProxyController.prototype, "chatCompletions", null);
__decorate([
    (0, common_1.Get)('api/tags'),
    __metadata("design:type", Function),
    __metadata("design:paramtypes", []),
    __metadata("design:returntype", Promise)
], OllamaProxyController.prototype, "listModels", null);
__decorate([
    (0, common_1.Get)('api/version'),
    __metadata("design:type", Function),
    __metadata("design:paramtypes", []),
    __metadata("design:returntype", Promise)
], OllamaProxyController.prototype, "getVersion", null);
__decorate([
    (0, common_1.All)('api/*'),
    __param(0, (0, common_1.Body)()),
    __param(1, (0, common_1.Req)()),
    __metadata("design:type", Function),
    __metadata("design:paramtypes", [Object, Object]),
    __metadata("design:returntype", Promise)
], OllamaProxyController.prototype, "debugCatchAll", null);
exports.OllamaProxyController = OllamaProxyController = OllamaProxyController_1 = __decorate([
    (0, common_1.Controller)(),
    __metadata("design:paramtypes", [rag_service_1.RagService])
], OllamaProxyController);
//# sourceMappingURL=ollama-proxy.controller.js.map