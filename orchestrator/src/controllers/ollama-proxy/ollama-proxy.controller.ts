import { Controller, Post, Body, Logger, Get, All, Req } from '@nestjs/common';
import { RagService } from 'src/services/rag/rag.service';
import { OllamaUtil } from 'src/utils/ollama.util';

@Controller() // SIN PREFIJO
export class OllamaProxyController {
  private readonly logger = new Logger(OllamaProxyController.name);

  constructor(private readonly ragService: RagService) {}

  /**
   * POST /api/generate - Formato Ollama directo
   */
  @Post('api/generate')
  async generate(@Body() body: any) {
    try {
      this.logger.log(`=== OLLAMA GENERATE REQUEST ===`);
      this.logger.log(`Request body: ${JSON.stringify(body, null, 2)}`);

      const prompt = body.prompt || '';
      const model = body.model || 'llama3.1:8b';

      if (!prompt) {
        throw new Error('No prompt found');
      }

      // Procesar con RAG
      const result = await this.ragService.processQuery(
        prompt,
        5, // maxResults
        0.3, // minScore
        model,
      );

      // Formato Ollama
      const response = {
        model: model,
        created_at: new Date().toISOString(),
        response: result.response,
        done: true,
        context: result.sources.map(s => s.payload.fileName),
        total_duration: result.processingTime * 1000000,
        load_duration: 0,
        prompt_eval_count: prompt.length,
        prompt_eval_duration: 1000000,
        eval_count: result.response.length,
        eval_duration: result.processingTime * 1000000,
      };

      this.logger.log(`=== SUCCESS ===`);
      this.logger.log(`Sources found: ${result.sources.length}`);
      return response;

    } catch (error) {
      this.logger.error('=== ERROR ===', error.message);
      return {
        model: body.model || 'llama3.1:8b',
        created_at: new Date().toISOString(),
        response: `Error: ${error.message}`,
        done: true,
        error: error.message,
      };
    }
  }

  /**
   * POST /api/chat - El endpoint que OpenWebUI realmente usa
   */
  @Post('api/chat')
  async chat(@Body() body: any, @Req() req: any) {
    try {
      const messages = body.messages || [];
      const lastMessage = messages[messages.length - 1];
      const prompt = lastMessage?.content || '';
      const model = body.model || 'llama3.1:8b';

      // FILTRAR requests automáticos de OpenWebUI
      const isAutoGenerated = prompt.includes('### Task:') || 
                             prompt.includes('follow-up questions') ||
                             prompt.includes('Generate a concise') ||
                             prompt.includes('Generate 1-3 broad tags');

      if (isAutoGenerated) {
        this.logger.log(`=== SKIPPING AUTO-GENERATED REQUEST ===`);
        this.logger.log(`Type: ${prompt.substring(0, 50)}...`);
        
        // Respuesta simple para requests automáticos
        return {
          model: model,
          created_at: new Date().toISOString(),
          response: '{}', // JSON vacío para auto-requests
          done: true,
          total_duration: 100000000,
        };
      }

      this.logger.log(`=== API CHAT REQUEST (USER) ===`);
      this.logger.log(`Prompt: "${prompt}"`);
      this.logger.log(`Stream: ${body.stream}`);

      if (!prompt) {
        throw new Error('No prompt found in messages');
      }

      // Procesar con RAG
      const result = await this.ragService.processQuery(
        prompt,
        5, // maxResults
        0.3, // minScore
        model,
      );

      // Para streaming, necesitamos formato especial
      if (body.stream) {
        // OpenWebUI espera este formato exacto para streaming
        const streamResponse = {
          model: model,
          created_at: new Date().toISOString(),
          message: {
            role: 'assistant',
            content: result.response,
          },
          done: true,
          total_duration: result.processingTime * 1000000,
          load_duration: 0,
          prompt_eval_count: prompt.length,
          prompt_eval_duration: 1000000,
          eval_count: result.response.length,
          eval_duration: result.processingTime * 1000000,
        };

        this.logger.log(`=== STREAMING SUCCESS ===`);
        this.logger.log(`Response: ${result.response.substring(0, 100)}...`);
        this.logger.log(`Sources: ${result.sources.length}`);
        return streamResponse;
      }

      // Respuesta no-stream (formato Ollama estándar)
      const ollamaResponse = {
        model: model,
        created_at: new Date().toISOString(),
        response: result.response,
        done: true,
        context: result.sources.map(s => s.payload.fileName),
        total_duration: result.processingTime * 1000000,
        load_duration: 0,
        prompt_eval_count: prompt.length,
        prompt_eval_duration: 1000000,
        eval_count: result.response.length,
        eval_duration: result.processingTime * 1000000,
      };

      this.logger.log(`=== NON-STREAMING SUCCESS ===`);
      this.logger.log(`Response: ${result.response.substring(0, 100)}...`);
      this.logger.log(`Sources: ${result.sources.length}`);
      return ollamaResponse;

    } catch (error) {
      this.logger.error('=== ERROR IN API CHAT ===', error.message);
      
      return {
        model: body.model || 'llama3.1:8b',
        created_at: new Date().toISOString(),
        response: `Error: ${error.message}`,
        done: true,
        error: error.message,
      };
    }
  }

  /**
   * POST /api/chat/completed - Endpoint de completado
   */
  @Post('api/chat/completed')
  async chatCompleted(@Body() body: any) {
    this.logger.log(`=== CHAT COMPLETED ===`);
    return { status: 'completed' };
  }

  /**
   * POST /api/chat/completions - OpenAI format para OpenWebUI
   */
  @Post('api/chat/completions')
  async chatCompletions(@Body() body: any, @Req() req: any) {
    try {
      this.logger.log(`=== OPENAI CHAT COMPLETIONS ===`);
      this.logger.log(`Request body: ${JSON.stringify(body, null, 2)}`);
      this.logger.log(`Stream: ${body.stream}`);

      // Extraer prompt del formato OpenAI
      const messages = body.messages || [];
      const lastMessage = messages[messages.length - 1];
      const prompt = lastMessage?.content || '';
      const model = body.model || 'llama3.1:8b';

      if (!prompt) {
        throw new Error('No prompt found in messages');
      }

      // Procesar con RAG
      const result = await this.ragService.processQuery(
        prompt,
        5, // maxResults
        0.3, // minScore
        model,
      );

      // OpenWebUI espera formato Ollama, no OpenAI streaming
      const ollamaResponse = {
        model: model,
        created_at: new Date().toISOString(),
        response: result.response,
        done: true,
        context: result.sources.map(s => s.payload.fileName),
        total_duration: result.processingTime * 1000000,
        load_duration: 0,
        prompt_eval_count: prompt.length,
        prompt_eval_duration: 1000000,
        eval_count: result.response.length,
        eval_duration: result.processingTime * 1000000,
      };

      this.logger.log(`=== CHAT COMPLETIONS SUCCESS ===`);
      this.logger.log(`Sources: ${result.sources.length}`);
      return ollamaResponse;

    } catch (error) {
      this.logger.error('=== ERROR IN CHAT COMPLETIONS ===', error.message);
      
      return {
        model: body.model || 'llama3.1:8b',
        created_at: new Date().toISOString(),
        response: `Error: ${error.message}`,
        done: true,
        error: error.message,
      };
    }
  }

  /**
   * GET /api/tags - Modelos disponibles  
   */
  @Get('api/tags')
  async listModels() {
    try {
      this.logger.log(`=== GETTING MODELS ===`);
      const models = await OllamaUtil.listModels();
      const response = {
        models: models.map(name => ({
          name,
          model: name,
          size: 4000000000,
          digest: 'sha256:fake',
          details: { format: 'gguf', family: 'llama' },
          modified_at: new Date().toISOString(),
        })),
      };
      this.logger.log(`=== MODELS RESPONSE ===`);
      this.logger.log(`Found ${models.length} models`);
      return response;
    } catch (error) {
      this.logger.error('=== ERROR GETTING MODELS ===', error);
      return { models: [] };
    }
  }

  /**
   * GET /api/version - Version info
   */
  @Get('api/version')
  async getVersion() {
    this.logger.log(`=== VERSION REQUEST ===`);
    return { version: '0.1.0-nestjs-rag' };
  }

  /**
   * Catch-all para debugging - DEBE IR AL FINAL
   */
  @All('api/*')
  async debugCatchAll(@Body() body: any, @Req() req: any) {
    this.logger.log(`=== CATCH-ALL API REQUEST ===`);
    this.logger.log(`Method: ${req.method}`);
    this.logger.log(`URL: ${req.url}`);
    this.logger.log(`Path: ${req.path}`);
    this.logger.log(`Body: ${JSON.stringify(body, null, 2)}`);
    
    return {
      message: 'Caught by NestJS catch-all - endpoint not found',
      method: req.method,
      url: req.url,
      path: req.path,
      body: body,
    };
  }
}